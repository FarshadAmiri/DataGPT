{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import accelerate\n",
    "import torch\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring text generation model, tokenizer, computational device and optional streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070 SUPER'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting device\n",
    "gpu=0\n",
    "device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name and hf token\n",
    "name = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "# name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "\n",
    "# hugginf face auth token\n",
    "# file_path = \"../../huggingface_credentials.txt\"\n",
    "# with open(file_path, \"r\") as file:\n",
    "#     auth_token = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name\n",
    "    # ,cache_dir='./model/'\n",
    "    # ,use_auth_token=auth_token\n",
    "    ,device_map='cuda'                 \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: D:\\NLP 1\\venv\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary D:\\NLP 1\\venv\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = AutoModelForCausalLM.from_pretrained(name\n",
    "    ,cache_dir=r\"C:\\Users\\user2\\.cache\\huggingface\\hub\"\n",
    "    # ,cache_dir='./model/'\n",
    "    # ,use_auth_token=auth_token\n",
    "    ,device_map='cuda'  \n",
    "    # , torch_dtype=torch.float16\n",
    "    # ,low_cpu_mem_usage=True\n",
    "    # ,rope_scaling={\"type\": \"dynamic\", \"factor\": 2}\n",
    "    # ,load_in_8bit=True,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_inference(plain_text, model, tokenizer, device, streamer=None, max_length=4000, ):\n",
    "    input_ids = tokenizer(\n",
    "        plain_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        )['input_ids'].to(device)\n",
    "    \n",
    "    output_ids = model.generate(input_ids\n",
    "                        ,streamer=streamer\n",
    "                        ,use_cache=True\n",
    "                        ,max_new_tokens=float('inf')\n",
    "                       )\n",
    "    answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating texts using a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NLP 1\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"what are the steps to train a machine learning model? explain in less than 100 words.\\nTo train a machine learning model, you typically follow these steps:\\n1. Collect and preprocess data.\\n2. Choose a machine learning algorithm.\\n3. Split the data into training and validation sets.\\n4. Train the model on the training set.\\n5. Evaluate the model's performance on the validation set.\\n6. Fine-tune the model as needed.\\n7. Test the final model on new data.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"what are the steps to train a machine learning model? explain in less than 100 words\"\n",
    "res = llm_inference(text, model, tokenizer, device, streamer=streamer,)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, get_response_synthesizer\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import VectorStoreIndex, download_loader\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=r\"../../vector_dbs/admin/vdb_satellite2\")\n",
    "\n",
    "# get collection\n",
    "chroma_collection = db.get_or_create_collection(\"default\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a system prompt\n",
    "system_prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as\n",
    "helpfully as possible, while being safe.\n",
    "If a question does not make any sense, or is not factually coherent, explain\n",
    "why instead of answering something not correct. If you don't know the answer\n",
    "to a question, please express that you do not have informaion or knowledge in\n",
    "that context and please don't share false information.\n",
    "Try to be exact in information and numbers you tell.\n",
    "Your goal is to provide answers based on the information provided and your other\n",
    "knowledge.<</SYS>>\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceLLM(context_window=4096,\n",
    "                     max_new_tokens=512,\n",
    "                     system_prompt=system_prompt,\n",
    "                     query_wrapper_prompt=query_wrapper_prompt,\n",
    "                     model=model,\n",
    "                     tokenizer=tokenizer)\n",
    "\n",
    "embeddings = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new service context instance\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    "    llm=llm,\n",
    "    embed_model=embeddings\n",
    ")\n",
    "\n",
    "# And set the service context\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert a single document into the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
    "loader = PyMuPDFReader()\n",
    "\n",
    "# Load documents\n",
    "# doc_dir = r\"D:\\NLP 1\\RAG-webapp\\documents_db\\Sattelite imagery article scripts.pdf\"\n",
    "doc_dir = r\"D:\\NLP 1\\RAG-webapp\\documents_db\\CLIMATE_CHANGE_2023.pdf\"\n",
    "document = loader.load(file_path=Path(doc_dir), metadata=False)\n",
    "\n",
    "# Create indexes\n",
    "for doc in document:\n",
    "    index.insert(doc, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert directory of documents into the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some documents\n",
    "documents = SimpleDirectoryReader(r\"C:\\Users\\user2\\Desktop\\RAG_Docs\").load_data()\n",
    "\n",
    "# create your index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "########## Or ###########\n",
    "#Customizing query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(streaming=True)\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.0)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NLP 1\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are several ship detection methods, including:\n",
      "1. Ship detection using satellite imagery with machine learning models\n",
      "2. Contrastive self-supervised learning for few-shot transfer learning\n",
      "3. Sentinel-2 ship detection dataset\n",
      "4. Unsupervised learning of features extractors for few-shot learning\n",
      "5. Deep learning-based approaches\n",
      "6. Classical image processing pipelines\n",
      "7. Object detection convolutional neural networks (CNN)</s>"
     ]
    }
   ],
   "source": [
    "# create a query engine and query\n",
    "# response = query_engine.query(\"who studied Master of Science in Management with a background in Civil Engineering?\")\n",
    "# response = query_engine.query(\"how many gold medals Iranian youth won in 2023 chess competitions?\")\n",
    "# response = query_engine.query(\"describe key points of 2023 climate change?\")\n",
    "# response = query_engine.query(\"how much headline inflation increased after storm shock?\")\n",
    "response = query_engine.query(\"how many ship detection methods are there? just name and use no more than 70 words\")\n",
    "# response = query_engine.query(\"say something\")\n",
    "# response = query_engine.query(\"how many gold medals Iranian youth won in 2023 chess competitions?\")\n",
    "response.print_response_stream()\n",
    "# ans = []\n",
    "# for txt in response.response_gen:\n",
    "#     ans.append(txt)\n",
    "#     print(txt, sep=\"\")\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NodeWithScore(node=TextNode(id_='901d0b5c-0aec-43ae-9424-595ccd14585f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b223a09c-d050-4e73-8603-644c02632f83', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='b61bb83fa485af0012e8b4018b648acb73bb7032c71d9fc5ed05b97d67fa6f9d')}, hash='44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a', text='as digital photographs), or by pretraining a neural network on the satellite image domain. The latter can be \\ndone through an unsupervised pipeline using self-supervised learning (SSL) [6], a contrastive learning \\nparadigm that extracts useful patterns, learns invariances and disentangles causal factors in the training data. \\nFeatures learned this way are better adapted for transfer learning of few-shot object detectors. We propose \\nto use this paradigm to create a ship detector with few data. \\n \\nFor VHR images, a large amount of literature exists, with the number of works follow- ing the increasing \\nnumber of sensors and the quantity of publicly available data [7,8]. Many of these approaches focused on \\ndetecting ships with classical image processing pipelines: image processing using spectral indices or histograms \\n(e.g., sea-land segmentation, cloud removal), ship candidate extraction (e.g., threshold, anomaly detection, \\nsaliency), and, then, rule-based ship identification or classification using statistical methods. Virtually \\nall of these works focus on VHR images with R,G,B, and PAN bands, occasionally with the addition of \\nNIR, with resolution less than 5 m. Deep learning was applied to images with under lm resolution by using \\nobject detection convolutional neural networks (CNN): R-CNNs [9,10], YOLO [11,12], U-Net \\n \\nFor SAR imagery, [1] reviews four operational ship detectors that work on multiple sensors. All of the \\napproaches use classical processing chains and start by filtering out land pixels. This filter is either based on \\nshapefiles or on land/water segmentation masks generated from the SAR image. However, in both cases, a large \\nmargin is taken around the coastlines, eliminating any ships that are moored in ports. Deep learning was also \\napplied to SAR ship detection, with notable results detailed in [15]. \\nIn multi-spectral images, the most notable work is [4] which uses SVMs to identify water, cloud, and land \\npixels and then builds a CNN to fuse multiple spectral channels. This fusion network predicts whether \\nobjects in the water are ships. Other approaches, such as [3], rely on hand made rules on size and spectral \\nvalues to distinguish between ships, clouds, islands, and icebergs. The only Sentinel 2 ship dataset \\npublicly available is [16] but it only includes small size image chips and weak annotations for precise \\nlocalization, \\n1. e., a single point for each ship, obtained by geo-referencing AIS GPS coordinates to pixel coordinates in \\nthe chips. \\nAlthough large datasets exist for VHR images, for Sentinel-2 none are available with pixel level \\nannotations while usually thousands of examples are needed to train deep learning object detectors. Few-\\nshot learning based approaches can bring interesting perspectives for remote sensing in general and in our \\nsetting in particular. Few-shot learning consists in training a neural network with few labeled samples, \\nmost often thanks to quality feature extractors upon which transfer learning is performed. One recent method \\nfor unsupervised learning of features extractors that enable few-shot learning is contrastive self-supervised \\nlearning [6,17]. Contrastive SSL relies on a \"pretext training task\", defined by the practitioner, that helps \\nthe network to learn invariances and latent patterns in the data [18-20]. \\nSeveral strategies exist for choosing the pretext task: context prediction [21], jigsaw puzzle, or simply by \\nconsidering various augmented views. The latter is used by [22,23] for remote sensing applications like land \\nuse classification and change detection. \\nContributions \\nIn this work, we make two contributions: \\n(1)  \\nA deep learning pipeline for ship detection with few training \\nexamples. We take advantage of self-supervised learning to learn features on large non-annotated datasets \\nof Sentinel 2 images and we learn a ship detector using few-shot transfer learning; \\n(2)  \\nA novel Sentinel 2 ship detection dataset, with 16 images of harbours \\nwith a total of 1053 ship annotations at the pixel level. \\n2. Materials and Methods \\nOur approach is based on a U-Net architecture with a ResNet-50 backbone to produce binary ship/no-ship', start_char_idx=0, end_char_idx=4189, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.36622370828842393),\n",
      " NodeWithScore(node=TextNode(id_='2ba47058-556c-495d-8e3e-97554d863b62', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e321c9ed-a609-477b-9ac2-6de365c2b391', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='a3e9daa4a16d6bfe5ed56b01c1b50ad20345380f87c6a8d5c226686d02a87566')}, hash='44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a', text='All of the articles bellow are discussing Ship detection using satellite imagery with Machine \\nlearning models and AI – our purpose is to provide beneficial information in order to \\nconduct research on the ship detection topic:  \\nArticle 1: \\nName of Article: Ship detection on Sentinel-2 images with Mask R-CNN model \\nAuthor: Andrea C. \\n \\nA time analysis of maritime traffic using PyTorch and open data \\nAs part of a larger ML project, we decided to explore the possibility to assess \\nmaritime traffic using publicly available satellite images. In particular, our goal \\nwas to estimate a time series that is representative of the volume of the maritime \\ntraffic observed over time in a given region. In this article, we discuss the \\nmethodology and results obtained. \\nNote: all code discussed in the following is available on my personal GitHub at \\nhttps://github.com/andrea-ci/s2-ship-detection. \\nWhy satellite data \\nPhoto by Matthijs van Heerikhuize on Unsplash \\nSatellite’s gone up to the skies \\nThings like that drive me out of my mind \\nI watched it for a little while \\nI like to watch things on TV \\nLou Reed, “Satellite of love” \\nIn the last years, remote sensing has dramatically evolved and so have other fields \\nsuch as computer vision and machine learning. In addition, more and more satellite \\nimagery has become publicly available. Notable examples include Landsat and \\nSentinel-2 constellation imagery. \\nSatellites exhibit three fundamental advantages for the users: \\nthey allow access to information that is often difficult to obtain by other means; \\nthey provide a global geographic coverage;', start_char_idx=0, end_char_idx=1606, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3474833078303168),\n",
      " NodeWithScore(node=TextNode(id_='e16582a1-eb34-45fa-ab6a-b7308dc91666', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ddb9f2fd-2670-43db-88f4-543205bd88c0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='faff90c706a8e98b3c76b131ecf971b2f245c4b9eb40ed9b7d4ddf0f2c311d7f')}, hash='44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a', text='procedure, we learn good features on Sentinel-2 images, without requiring labeling, to initialize our network\\'s \\nbackbone. The full net- work is then fine-tuned to learn to detect ships in challenging settings. We evaluated this \\napproach versus pre-training on ImageNet and versus a classical image processing pipeline. We examined the impact \\nof variations in the self-supervised learning step and we show that in the few-shot learning setting self-supervised \\npre-training achieves better results than ImageNet pre-training. When enough training data are available , our self -\\nsupervised approach is as good as ImageNet pre-training . We conclude that a better design of the self-supervised \\ntask and bigger non-annotated dataset sizes can lead to surpassing ImageNet pre-training performance without any \\nannotation costs. \\n1. Introduction \\nShip detection is an important challenge in economic intelligence and maritime secu- rity, with \\napplications in detecting piracy or illegal fishing and monitoring logistic chains. For now, cooperative \\ntransponders systems, such as AIS, provide ship detection and identification for maritime surveillance. However, \\nsome ships may have non-functioning transponders; many times they are turned off on purpose to hide ship \\nmovements. Mar- itime patrols can help to identify suspect ships, but this requires many resources and \\ntheir range is restricted. Therefore, using satellites, such as those from the European Space Agency Sentinel-\\n2 mission, to detect ships in littoral regions is a promising solution thanks to their large swath and high \\nrevisit time. \\nSome commercial satellite constellations offer very high resolution images (VHR) (<1 m/pixel) \\nwith low revisit time (1-2 days). However, VHR images are usually limited to the R, G, B bands and \\nimage analysis on such high resolution images is computationally intensive. On the other hand, synthetic \\naperture radar (SAR) satellites can also be used, although their resolution is lower than VHR optical \\nsources (e.g., Sentinel 1 has 5 m resolution), the analysis of their imagery is the main approach to \\nship detection since SAR images can be acquired irrespective of cloud cover and the day and night cycle. The \\ndownsides of SAR are low performance in rough sea conditions, but, most importantly, detection is only \\ndone on seas away from land and is not possible for moored ships in harbor or for ships smaller than 10 \\nm [1]. Furthermore, SAR is vulnerable to jamming [2]. \\n \\nThe Copernicus Sentinel-2 mission of the European Space Agency offers free multi- spectral images with a \\nrefresh rate of maximum 5 days and a resolution down to 10 m, as detailed in Table 1. Our work \\nfocuses on this data source for several reasons. First, multi-spectral information allows to better extract a ship \\nfingerprint and distinguish it from land or man-made structures, as shown in [3,4]. Second, a multi-spectral \\noptical learning based approach can perform detection in both high seas and harbor contexts, while \\nalso removing the requirement of storing a vector map of coastlines and performing cloud removal as a pre-\\nprocessing step. Thus, it could be adapted to a real-time, on-board satellite setting and is not affected by jamming. \\n \\n \\nAlthough ship detection is a challenging task, ship identification in remote-sensing images is even more \\ndifficult [5]. A coarse identification could be made by ship type (container ship, fishing vessel, barge, \\ncruiseliner, etc.) using supervised classification, with accuracy that should be closely related to image \\nresolution. However, to establish ship identity uniquely, it does not seem feasible with the Sentinel-2 sensor \\nto extract features fit for this purpose, such as measurement of ships to meter precision, extracting exact \\ncontours, or detecting salient unique traits of different ships. Our work focuses on detection but the \\napproach is generic and could be extended to other sensors with better resolution, eventually allowing \\nidentification. \\nRecent remote sensing approaches based on machine learning require large amounts of annotated data. Some \\nefforts to collect and annotate data have been made for VHR images, for SAR and for Sentinel 2, but, for the \\nlatter, these works did not target ship detection in particular. For object detection using convolutional neural \\nnetworks (CNN), an interesting way to overcome the lack of data is to use transfer learning. This is achieved \\neither by using CNNs pretrained on large labeled data sets gathered in a sufficiently \"close\" domain (such \\nhigh-resolution', start_char_idx=0, end_char_idx=4578, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.33616504860817364)]\n"
     ]
    }
   ],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('as digital photographs), or by pretraining a neural network on the satellite '\n",
      " 'image domain. The latter can be \\n'\n",
      " 'done through an unsupervised pipeline using self-supervised learning (SSL) '\n",
      " '[6], a contrastive learning \\n'\n",
      " 'paradigm that extracts useful patterns, learns invariances and disentangles '\n",
      " 'causal factors in the training data. \\n'\n",
      " 'Features learned this way are better adapted for transfer learning of '\n",
      " 'few-shot object detectors. We propose \\n'\n",
      " 'to use this paradigm to create a ship detector with few data. \\n'\n",
      " ' \\n'\n",
      " 'For VHR images, a large amount of literature exists, with the number of '\n",
      " 'works follow- ing the increasing \\n'\n",
      " 'number of sensors and the quantity of publicly available data [7,8]. Many of '\n",
      " 'these approaches focused on \\n'\n",
      " 'detecting ships with classical image processing pipelines: image processing '\n",
      " 'using spectral indices or histograms \\n'\n",
      " '(e.g., sea-land segmentation, cloud removal), ship candidate extraction '\n",
      " '(e.g., threshold, anomaly detection, \\n'\n",
      " 'saliency), and, then, rule-based ship identification or classification using '\n",
      " 'statistical methods. Virtually \\n'\n",
      " 'all of these works focus on VHR images with R,G,B, and PAN bands, '\n",
      " 'occasionally with the addition of \\n'\n",
      " 'NIR, with resolution less than 5 m. Deep learning was applied to images with '\n",
      " 'under lm resolution by using \\n'\n",
      " 'object detection convolutional neural networks (CNN): R-CNNs [9,10], YOLO '\n",
      " '[11,12], U-Net \\n'\n",
      " ' \\n'\n",
      " 'For SAR imagery, [1] reviews four operational ship detectors that work on '\n",
      " 'multiple sensors. All of the \\n'\n",
      " 'approaches use classical processing chains and start by filtering out land '\n",
      " 'pixels. This filter is either based on \\n'\n",
      " 'shapefiles or on land/water segmentation masks generated from the SAR image. '\n",
      " 'However, in both cases, a large \\n'\n",
      " 'margin is taken around the coastlines, eliminating any ships that are moored '\n",
      " 'in ports. Deep learning was also \\n'\n",
      " 'applied to SAR ship detection, with notable results detailed in [15]. \\n'\n",
      " 'In multi-spectral images, the most notable work is [4] which uses SVMs to '\n",
      " 'identify water, cloud, and land \\n'\n",
      " 'pixels and then builds a CNN to fuse multiple spectral channels. This fusion '\n",
      " 'network predicts whether \\n'\n",
      " 'objects in the water are ships. Other approaches, such as [3], rely on hand '\n",
      " 'made rules on size and spectral \\n'\n",
      " 'values to distinguish between ships, clouds, islands, and icebergs. The only '\n",
      " 'Sentinel 2 ship dataset \\n'\n",
      " 'publicly available is [16] but it only includes small size image chips and '\n",
      " 'weak annotations for precise \\n'\n",
      " 'localization, \\n'\n",
      " '1. e., a single point for each ship, obtained by geo-referencing AIS GPS '\n",
      " 'coordinates to pixel coordinates in \\n'\n",
      " 'the chips. \\n'\n",
      " 'Although large datasets exist for VHR images, for Sentinel-2 none are '\n",
      " 'available with pixel level \\n'\n",
      " 'annotations while usually thousands of examples are needed to train deep '\n",
      " 'learning object detectors. Few-\\n'\n",
      " 'shot learning based approaches can bring interesting perspectives for remote '\n",
      " 'sensing in general and in our \\n'\n",
      " 'setting in particular. Few-shot learning consists in training a neural '\n",
      " 'network with few labeled samples, \\n'\n",
      " 'most often thanks to quality feature extractors upon which transfer learning '\n",
      " 'is performed. One recent method \\n'\n",
      " 'for unsupervised learning of features extractors that enable few-shot '\n",
      " 'learning is contrastive self-supervised \\n'\n",
      " 'learning [6,17]. Contrastive SSL relies on a \"pretext training task\", '\n",
      " 'defined by the practitioner, that helps \\n'\n",
      " 'the network to learn invariances and latent patterns in the data [18-20]. \\n'\n",
      " 'Several strategies exist for choosing the pretext task: context prediction '\n",
      " '[21], jigsaw puzzle, or simply by \\n'\n",
      " 'considering various augmented views. The latter is used by [22,23] for '\n",
      " 'remote sensing applications like land \\n'\n",
      " 'use classification and change detection. \\n'\n",
      " 'Contributions \\n'\n",
      " 'In this work, we make two contributions: \\n'\n",
      " '(1)  \\n'\n",
      " 'A deep learning pipeline for ship detection with few training \\n'\n",
      " 'examples. We take advantage of self-supervised learning to learn features on '\n",
      " 'large non-annotated datasets \\n'\n",
      " 'of Sentinel 2 images and we learn a ship detector using few-shot transfer '\n",
      " 'learning; \\n'\n",
      " '(2)  \\n'\n",
      " 'A novel Sentinel 2 ship detection dataset, with 16 images of harbours \\n'\n",
      " 'with a total of 1053 ship annotations at the pixel level. \\n'\n",
      " '2. Materials and Methods \\n'\n",
      " 'Our approach is based on a U-Net architecture with a ResNet-50 backbone to '\n",
      " 'produce binary ship/no-ship')\n"
     ]
    }
   ],
   "source": [
    "pprint(response.source_nodes[0].node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in response.response_gen:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\NLP 1\\RAG-webapp\\main\\utilities\\RAG_with_Vectordb.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NLP%201/RAG-webapp/main/utilities/RAG_with_Vectordb.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39msource_nodes:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP%201/RAG-webapp/main/utilities/RAG_with_Vectordb.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(node\u001b[39m.\u001b[39mscore)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP%201/RAG-webapp/main/utilities/RAG_with_Vectordb.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m response\u001b[39m.\u001b[39msource_nodes[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "for node in response.source_nodes:\n",
    "    print(node.score)\n",
    "response.source_nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
