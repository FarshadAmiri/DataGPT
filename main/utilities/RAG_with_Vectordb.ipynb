{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import accelerate\n",
    "import torch\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring text generation model, tokenizer, computational device and optional streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070 SUPER'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting device\n",
    "gpu=0\n",
    "device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name and hf token\n",
    "name = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "# name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "\n",
    "# hugginf face auth token\n",
    "# file_path = \"../../huggingface_credentials.txt\"\n",
    "# with open(file_path, \"r\") as file:\n",
    "#     auth_token = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name\n",
    "    # ,cache_dir='./model/'\n",
    "    # ,use_auth_token=auth_token\n",
    "    ,device_map='cuda'                 \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: D:\\NLP 1\\venv\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary D:\\NLP 1\\venv\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = AutoModelForCausalLM.from_pretrained(name\n",
    "    ,cache_dir=r\"C:\\Users\\user2\\.cache\\huggingface\\hub\"\n",
    "    # ,cache_dir='./model/'\n",
    "    # ,use_auth_token=auth_token\n",
    "    ,device_map='cuda'  \n",
    "    # , torch_dtype=torch.float16\n",
    "    # ,low_cpu_mem_usage=True\n",
    "    # ,rope_scaling={\"type\": \"dynamic\", \"factor\": 2}\n",
    "    # ,load_in_8bit=True,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_inference(plain_text, model, tokenizer, device, streamer=None, max_length=4000, ):\n",
    "    input_ids = tokenizer(\n",
    "        plain_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        )['input_ids'].to(device)\n",
    "    \n",
    "    output_ids = model.generate(input_ids\n",
    "                        ,streamer=streamer\n",
    "                        ,use_cache=True\n",
    "                        ,max_new_tokens=float('inf')\n",
    "                       )\n",
    "    answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating texts using a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NLP 1\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"what are the steps to train a machine learning model? explain in less than 100 words.\\nTo train a machine learning model, you typically follow these steps:\\n1. Collect and preprocess data.\\n2. Choose a machine learning algorithm.\\n3. Split the data into training and validation sets.\\n4. Train the model on the training set.\\n5. Evaluate the model's performance on the validation set.\\n6. Fine-tune the model as needed.\\n7. Test the final model on new data.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"what are the steps to train a machine learning model? explain in less than 100 words\"\n",
    "res = llm_inference(text, model, tokenizer, device, streamer=streamer,)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, get_response_synthesizer\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import VectorStoreIndex, download_loader\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=r\"../../vector_dbs/admin/vdb_satellite2\")\n",
    "\n",
    "# get collection\n",
    "chroma_collection = db.get_or_create_collection(\"default\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a system prompt\n",
    "system_prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as\n",
    "helpfully as possible, while being safe.\n",
    "If a question does not make any sense, or is not factually coherent, explain\n",
    "why instead of answering something not correct. If you don't know the answer\n",
    "to a question, please express that you do not have informaion or knowledge in\n",
    "that context and please don't share false information.\n",
    "Try to be exact in information and numbers you tell.\n",
    "Your goal is to provide answers based on the information provided and your other\n",
    "knowledge.<</SYS>>\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceLLM(context_window=4096,\n",
    "                     max_new_tokens=512,\n",
    "                     system_prompt=system_prompt,\n",
    "                     query_wrapper_prompt=query_wrapper_prompt,\n",
    "                     model=model,\n",
    "                     tokenizer=tokenizer)\n",
    "\n",
    "embeddings = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new service context instance\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    "    llm=llm,\n",
    "    embed_model=embeddings\n",
    ")\n",
    "\n",
    "# And set the service context\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert a single document into the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
    "loader = PyMuPDFReader()\n",
    "\n",
    "# Load documents\n",
    "# doc_dir = r\"D:\\NLP 1\\RAG-webapp\\documents_db\\Sattelite imagery article scripts.pdf\"\n",
    "doc_dir = r\"D:\\NLP 1\\RAG-webapp\\documents_db\\CLIMATE_CHANGE_2023.pdf\"\n",
    "document = loader.load(file_path=Path(doc_dir), metadata=False)\n",
    "\n",
    "# Create indexes\n",
    "for doc in document:\n",
    "    index.insert(doc, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert directory of documents into the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some documents\n",
    "documents = SimpleDirectoryReader(r\"C:\\Users\\user2\\Desktop\\RAG_Docs\").load_data()\n",
    "\n",
    "# create your index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "########## Or ###########\n",
    "#Customizing query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(streaming=True)\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.0)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NLP 1\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are several ship detection methods, including:\n",
      "1. Ship detection using satellite imagery with machine learning models\n",
      "2. Ship detection using SAR imagery with deep learning\n",
      "3. Ship detection using multi-spectral images with transfer learning\n",
      "4. Ship detection using VHR images with self-supervised learning\n",
      "5. Ship detection using Sentinel-2 images with few-shot learning\n",
      "6. Ship detection using land use and land cover classification\n",
      "7. Ship detection using change detection techniques.</s>"
     ]
    }
   ],
   "source": [
    "# create a query engine and query\n",
    "# response = query_engine.query(\"who studied Master of Science in Management with a background in Civil Engineering?\")\n",
    "# response = query_engine.query(\"how many gold medals Iranian youth won in 2023 chess competitions?\")\n",
    "# response = query_engine.query(\"describe key points of 2023 climate change?\")\n",
    "# response = query_engine.query(\"how much headline inflation increased after storm shock?\")\n",
    "response = query_engine.query(\"how many ship detection methods are there? just name and use no more than 70 words\")\n",
    "# response = query_engine.query(\"say something\")\n",
    "# response = query_engine.query(\"how many gold medals Iranian youth won in 2023 chess competitions?\")\n",
    "response.print_response_stream()\n",
    "# ans = []\n",
    "# for txt in response.response_gen:\n",
    "#     ans.append(txt)\n",
    "#     print(txt, sep=\"\")\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('as digital photographs), or by pretraining a neural network on the satellite '\n",
      " 'image domain. The latter can be \\n'\n",
      " 'done through an unsupervised pipeline using self-supervised learning (SSL) '\n",
      " '[6], a contrastive learning \\n'\n",
      " 'paradigm that extracts useful patterns, learns invariances and disentangles '\n",
      " 'causal factors in the training data. \\n'\n",
      " 'Features learned this way are better adapted for transfer learning of '\n",
      " 'few-shot object detectors. We propose \\n'\n",
      " 'to use this paradigm to create a ship detector with few data. \\n'\n",
      " ' \\n'\n",
      " 'For VHR images, a large amount of literature exists, with the number of '\n",
      " 'works follow- ing the increasing \\n'\n",
      " 'number of sensors and the quantity of publicly available data [7,8]. Many of '\n",
      " 'these approaches focused on \\n'\n",
      " 'detecting ships with classical image processing pipelines: image processing '\n",
      " 'using spectral indices or histograms \\n'\n",
      " '(e.g., sea-land segmentation, cloud removal), ship candidate extraction '\n",
      " '(e.g., threshold, anomaly detection, \\n'\n",
      " 'saliency), and, then, rule-based ship identification or classification using '\n",
      " 'statistical methods. Virtually \\n'\n",
      " 'all of these works focus on VHR images with R,G,B, and PAN bands, '\n",
      " 'occasionally with the addition of \\n'\n",
      " 'NIR, with resolution less than 5 m. Deep learning was applied to images with '\n",
      " 'under lm resolution by using \\n'\n",
      " 'object detection convolutional neural networks (CNN): R-CNNs [9,10], YOLO '\n",
      " '[11,12], U-Net \\n'\n",
      " ' \\n'\n",
      " 'For SAR imagery, [1] reviews four operational ship detectors that work on '\n",
      " 'multiple sensors. All of the \\n'\n",
      " 'approaches use classical processing chains and start by filtering out land '\n",
      " 'pixels. This filter is either based on \\n'\n",
      " 'shapefiles or on land/water segmentation masks generated from the SAR image. '\n",
      " 'However, in both cases, a large \\n'\n",
      " 'margin is taken around the coastlines, eliminating any ships that are moored '\n",
      " 'in ports. Deep learning was also \\n'\n",
      " 'applied to SAR ship detection, with notable results detailed in [15]. \\n'\n",
      " 'In multi-spectral images, the most notable work is [4] which uses SVMs to '\n",
      " 'identify water, cloud, and land \\n'\n",
      " 'pixels and then builds a CNN to fuse multiple spectral channels. This fusion '\n",
      " 'network predicts whether \\n'\n",
      " 'objects in the water are ships. Other approaches, such as [3], rely on hand '\n",
      " 'made rules on size and spectral \\n'\n",
      " 'values to distinguish between ships, clouds, islands, and icebergs. The only '\n",
      " 'Sentinel 2 ship dataset \\n'\n",
      " 'publicly available is [16] but it only includes small size image chips and '\n",
      " 'weak annotations for precise \\n'\n",
      " 'localization, \\n'\n",
      " '1. e., a single point for each ship, obtained by geo-referencing AIS GPS '\n",
      " 'coordinates to pixel coordinates in \\n'\n",
      " 'the chips. \\n'\n",
      " 'Although large datasets exist for VHR images, for Sentinel-2 none are '\n",
      " 'available with pixel level \\n'\n",
      " 'annotations while usually thousands of examples are needed to train deep '\n",
      " 'learning object detectors. Few-\\n'\n",
      " 'shot learning based approaches can bring interesting perspectives for remote '\n",
      " 'sensing in general and in our \\n'\n",
      " 'setting in particular. Few-shot learning consists in training a neural '\n",
      " 'network with few labeled samples, \\n'\n",
      " 'most often thanks to quality feature extractors upon which transfer learning '\n",
      " 'is performed. One recent method \\n'\n",
      " 'for unsupervised learning of features extractors that enable few-shot '\n",
      " 'learning is contrastive self-supervised \\n'\n",
      " 'learning [6,17]. Contrastive SSL relies on a \"pretext training task\", '\n",
      " 'defined by the practitioner, that helps \\n'\n",
      " 'the network to learn invariances and latent patterns in the data [18-20]. \\n'\n",
      " 'Several strategies exist for choosing the pretext task: context prediction '\n",
      " '[21], jigsaw puzzle, or simply by \\n'\n",
      " 'considering various augmented views. The latter is used by [22,23] for '\n",
      " 'remote sensing applications like land \\n'\n",
      " 'use classification and change detection. \\n'\n",
      " 'Contributions \\n'\n",
      " 'In this work, we make two contributions: \\n'\n",
      " '(1)  \\n'\n",
      " 'A deep learning pipeline for ship detection with few training \\n'\n",
      " 'examples. We take advantage of self-supervised learning to learn features on '\n",
      " 'large non-annotated datasets \\n'\n",
      " 'of Sentinel 2 images and we learn a ship detector using few-shot transfer '\n",
      " 'learning; \\n'\n",
      " '(2)  \\n'\n",
      " 'A novel Sentinel 2 ship detection dataset, with 16 images of harbours \\n'\n",
      " 'with a total of 1053 ship annotations at the pixel level. \\n'\n",
      " '2. Materials and Methods \\n'\n",
      " 'Our approach is based on a U-Net architecture with a ResNet-50 backbone to '\n",
      " 'produce binary ship/no-ship')\n"
     ]
    }
   ],
   "source": [
    "pprint(response.source_nodes[0].node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in response.response_gen:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\NLP 1\\RAG-webapp\\main\\utilities\\RAG_with_Vectordb.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NLP%201/RAG-webapp/main/utilities/RAG_with_Vectordb.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39msource_nodes:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP%201/RAG-webapp/main/utilities/RAG_with_Vectordb.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(node\u001b[39m.\u001b[39mscore)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP%201/RAG-webapp/main/utilities/RAG_with_Vectordb.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m response\u001b[39m.\u001b[39msource_nodes[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "for node in response.source_nodes:\n",
    "    print(node.score)\n",
    "response.source_nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
