{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6655628",
   "metadata": {},
   "source": [
    "Query-passage samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7e50359",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"How many people live in Berlin?\"\n",
    "passages_1 = [\n",
    "    \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\",\n",
    "    \"Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.\",\n",
    "    \"In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\",\n",
    "]\n",
    "\n",
    "query_2 = \"Explain the effects of railroads in Britain\"\n",
    "passages_2 = [\n",
    "    \"Railroads revolutionized transport and trade in Britain...\",\n",
    "    \"The industrial revolution and railroads helped...\",\n",
    "    \"Some unrelated text here...\"\n",
    "]\n",
    "\n",
    "query_3 = \"تاثیرات راه‌آهن‌ها در اقتصاد ایران چیست؟\"\n",
    "\n",
    "passages_3 = [\n",
    "    \"راه‌آهن‌ها باعث تسهیل حمل و نقل و توسعه اقتصادی در ایران شدند.\",\n",
    "    \"صنعت نفت در ایران تاثیر بسیار زیادی بر اقتصاد داشت.\",\n",
    "    \"ورزش فوتبال در ایران محبوبیت زیادی دارد و تأثیر فرهنگی زیادی داشته است.\",\n",
    "    \"راه‌آهن‌ها در دوران پهلوی نقش مهمی در گسترش تجارت و حمل‌ونقل ایفا کردند.\"\n",
    "]\n",
    "\n",
    "query_lg = \"explain effects of railorads in Britain... bery brief\"\n",
    "\n",
    "passages_lg = ['spurred industrial growth by giving manufacturers a cheap way to transport mate-rials and finished products. Second, the railroad boom created hundreds of thou-sands of new jobs for both railroad workers and miners. These miners providediron for the tracks and coal for the steam engines. Third, the railroads boostedEngland’ s agricultural and fishing industries, which could transport their productsto distant cities.\\nFinally, by making travel easier, railroads encouraged country people to take dis-\\ntant city jobs. Also, railroads lured city dwellers to resorts in the countryside. Likea locomotive racing across the country, the Industrial Revolution brought rapid andunsettling changes to people’ s lives.\\nTERMS & NAMES 1.For each term or name, write a sentence explaining its significance.\\n• Industrial Revolution • enclosure • crop rotation • industrialization • factors of production • factory • entrepreneur\\nUSING YOUR NOTES\\n2.Which of the events listed do\\nyou think was the most\\nimportant? Explain. (10.3.5)MAIN IDEAS\\n3.What were four factors that\\ncontributed to industrializationin Britain?\\n(10.3.1)\\n4.How did rising population helpthe Industrial Revolution?\\n(10.3.1)\\n5.What American invention\\naided the British textileindustry?\\n(10.3.1)SECTION ASSESSMENT 1\\nCREATING AN ILLUSTRATED NEWS ARTICLE\\nFind information on a recent agricultural or technological invention or improvement. Write a two-paragraph news article about its economic effects and include an illustration, if possible.\\n(10.3.5)CRITICAL THINKING & WRITING\\n6. EVALUATING Was the revolution in agriculture necessary\\nto the Industrial Revolution? Explain.',\n",
    " 'was due to their location along the nation’ s expanding railroad lines. Chicago’ s\\nstockyards and Minneapolis’ s grain industries prospered by selling products to the\\nrest of the country. Indeed, the railroads themselves proved to be a profitable busi-ness. By the end of the 1800s, a limited number of large, powerful companies con-trolled more than two-thirds of the nation’ s railroad tracks. Businesses of all kindsbegan to merge as the railroads had. Smaller companies joined together to form alarger one.\\nThe Rise of Corporations Building large businesses like railroads required a great\\ndeal of money. To raise the money, entrepreneurs sold shares of stock , or certain\\nrights of ownership. Thus people who bought stock became part owners of thesebusinesses, which were called corporations. A\\ncorporation is a business owned by\\nstockholders who share in its profits but are not personally responsible for itsdebts. Corporations were able to raise the large amounts of capital needed to investin industrial equipment.\\nIn the late 1800s, large corporations such as Standard Oil (founded by John D.\\nRockefeller) and the Carnegie Steel Company (founded by Andrew Carnegie)sprang up. They sought to control every aspect of their own industries in order tomake big profits. Big business—the giant corporations that controlled entire indus-tries—also made big profits by reducing the cost of producing goods. In the UnitedStates as elsewhere, workers earned low wages for laboring long hours, whilestockholders earned high profits and corporate leaders made fortunes.\\nContinental Europe Industrializes\\nEuropean businesses yearned to adopt the “British miracle,” the result of Britain’ sprofitable new methods of manufacturing goods.',\n",
    " 'Improvements in Transportation\\nProgress in the textile industry spurred other industrial improvements. The\\nfirst such development, the steam engine, stemmed from the search for a cheap,\\nconvenient source of power. As early as 1705, coal miners were using steam-powered pumps to remove water from deep mine shafts. But this early model of asteam engine gobbled great quantities of fuel, making it expensive to run.\\nWatt’s Steam Engine James Watt, a mathematical instrument maker at the\\nUniversity of Glasgow in Scotland, thought about the problem for two years. In1765, Watt figured out a way to make the steam engine work faster and more effi-ciently while burning less fuel. In 1774, Watt joined with a businessman namedMatthew Boulton. Boulton was an\\nentrepreneur (AHN•truh•pruh•NUR), a person\\nwho organizes, manages, and takes on the risks of a business. He paid Watt a salary\\nand encouraged him to build better engines.\\nWater Transportation Steam could also propel boats. An American inventor\\nnamed Robert Fulton ordered a steam engine from Boulton and Watt. He built asteamboat called the Clermont, which made its first successful trip in 1807. TheClermont later ferried passengers up and down New Y ork’ s Hudson River.\\nIn England, water transportation improved with the creation of a network of\\ncanals, or human-made waterways. By the mid-1800s, 4,250 miles of inland chan-nels slashed the cost of transporting both raw materials and finished goods.\\nRoad Transportation British roads improved, too, thanks largely to the efforts of\\nJohn McAdam, a Scottish engineer.',\n",
    "    \"راه‌آهن‌ها باعث تسهیل حمل و نقل و توسعه اقتصادی در ایران شدند.\",\n",
    "    \"راه‌آهن‌ها در دوران پهلوی نقش مهمی در گسترش تجارت و حمل‌ونقل ایفا کردند.\",\n",
    "    \"صنعت نفت در ایران تاثیر بسیار زیادی بر اقتصاد داشت.\",\n",
    "    \"ورزش فوتبال در ایران محبوبیت زیادی دارد و تأثیر فرهنگی زیادی داشته است.\",\n",
    "    \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\",\n",
    "    \"Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.\",\n",
    "    \"In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\",\n",
    " \n",
    " ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a3b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: explain effects of railorads in Britain... bery brief\n",
      "- #0 (-6.70): spurred industrial growth by giving manufacturers a cheap way to transport mate-rials and finished p...\n",
      "- #1 (-7.04): was due to their location along the nation’ s expanding railroad lines. Chicago’ s\n",
      "stockyards and Mi...\n",
      "- #2 (-10.36): Improvements in Transportation\n",
      "Progress in the textile industry spurred other industrial improvement...\n",
      "- #3 (-11.19): راه‌آهن‌ها باعث تسهیل حمل و نقل و توسعه اقتصادی در ایران شدند....\n",
      "- #5 (-11.22): صنعت نفت در ایران تاثیر بسیار زیادی بر اقتصاد داشت....\n",
      "- #6 (-11.24): ورزش فوتبال در ایران محبوبیت زیادی دارد و تأثیر فرهنگی زیادی داشته است....\n",
      "- #4 (-11.27): راه‌آهن‌ها در دوران پهلوی نقش مهمی در گسترش تجارت و حمل‌ونقل ایفا کردند....\n",
      "- #7 (-11.42): Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers....\n",
      "- #9 (-11.45): In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clu...\n",
      "- #8 (-11.46): Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited citie...\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# 1. Load a pretrained CrossEncoder model\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "\n",
    "query = query_lg\n",
    "passages = passages_lg\n",
    "\n",
    "# 2b. Or rank a list of passages for a query\n",
    "ranks = reranker.rank(query, passages, return_documents=True)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "for rank in ranks:\n",
    "    print(f\"- #{rank['corpus_id']} ({rank['score']:.2f}): {rank['text'][0:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3312a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 0,\n",
       "  'score': -6.701413,\n",
       "  'text': 'spurred industrial growth by giving manufacturers a cheap way to transport mate-rials and finished products. Second, the railroad boom created hundreds of thou-sands of new jobs for both railroad workers and miners. These miners providediron for the tracks and coal for the steam engines. Third, the railroads boostedEngland’ s agricultural and fishing industries, which could transport their productsto distant cities.\\nFinally, by making travel easier, railroads encouraged country people to take dis-\\ntant city jobs. Also, railroads lured city dwellers to resorts in the countryside. Likea locomotive racing across the country, the Industrial Revolution brought rapid andunsettling changes to people’ s lives.\\nTERMS & NAMES 1.For each term or name, write a sentence explaining its significance.\\n• Industrial Revolution • enclosure • crop rotation • industrialization • factors of production • factory • entrepreneur\\nUSING YOUR NOTES\\n2.Which of the events listed do\\nyou think was the most\\nimportant? Explain. (10.3.5)MAIN IDEAS\\n3.What were four factors that\\ncontributed to industrializationin Britain?\\n(10.3.1)\\n4.How did rising population helpthe Industrial Revolution?\\n(10.3.1)\\n5.What American invention\\naided the British textileindustry?\\n(10.3.1)SECTION ASSESSMENT 1\\nCREATING AN ILLUSTRATED NEWS ARTICLE\\nFind information on a recent agricultural or technological invention or improvement. Write a two-paragraph news article about its economic effects and include an illustration, if possible.\\n(10.3.5)CRITICAL THINKING & WRITING\\n6. EVALUATING Was the revolution in agriculture necessary\\nto the Industrial Revolution? Explain.'},\n",
       " {'corpus_id': 1,\n",
       "  'score': -7.0381074,\n",
       "  'text': 'was due to their location along the nation’ s expanding railroad lines. Chicago’ s\\nstockyards and Minneapolis’ s grain industries prospered by selling products to the\\nrest of the country. Indeed, the railroads themselves proved to be a profitable busi-ness. By the end of the 1800s, a limited number of large, powerful companies con-trolled more than two-thirds of the nation’ s railroad tracks. Businesses of all kindsbegan to merge as the railroads had. Smaller companies joined together to form alarger one.\\nThe Rise of Corporations Building large businesses like railroads required a great\\ndeal of money. To raise the money, entrepreneurs sold shares of stock , or certain\\nrights of ownership. Thus people who bought stock became part owners of thesebusinesses, which were called corporations. A\\ncorporation is a business owned by\\nstockholders who share in its profits but are not personally responsible for itsdebts. Corporations were able to raise the large amounts of capital needed to investin industrial equipment.\\nIn the late 1800s, large corporations such as Standard Oil (founded by John D.\\nRockefeller) and the Carnegie Steel Company (founded by Andrew Carnegie)sprang up. They sought to control every aspect of their own industries in order tomake big profits. Big business—the giant corporations that controlled entire indus-tries—also made big profits by reducing the cost of producing goods. In the UnitedStates as elsewhere, workers earned low wages for laboring long hours, whilestockholders earned high profits and corporate leaders made fortunes.\\nContinental Europe Industrializes\\nEuropean businesses yearned to adopt the “British miracle,” the result of Britain’ sprofitable new methods of manufacturing goods.'},\n",
       " {'corpus_id': 2,\n",
       "  'score': -10.36473,\n",
       "  'text': 'Improvements in Transportation\\nProgress in the textile industry spurred other industrial improvements. The\\nfirst such development, the steam engine, stemmed from the search for a cheap,\\nconvenient source of power. As early as 1705, coal miners were using steam-powered pumps to remove water from deep mine shafts. But this early model of asteam engine gobbled great quantities of fuel, making it expensive to run.\\nWatt’s Steam Engine James Watt, a mathematical instrument maker at the\\nUniversity of Glasgow in Scotland, thought about the problem for two years. In1765, Watt figured out a way to make the steam engine work faster and more effi-ciently while burning less fuel. In 1774, Watt joined with a businessman namedMatthew Boulton. Boulton was an\\nentrepreneur (AHN•truh•pruh•NUR), a person\\nwho organizes, manages, and takes on the risks of a business. He paid Watt a salary\\nand encouraged him to build better engines.\\nWater Transportation Steam could also propel boats. An American inventor\\nnamed Robert Fulton ordered a steam engine from Boulton and Watt. He built asteamboat called the Clermont, which made its first successful trip in 1807. TheClermont later ferried passengers up and down New Y ork’ s Hudson River.\\nIn England, water transportation improved with the creation of a network of\\ncanals, or human-made waterways. By the mid-1800s, 4,250 miles of inland chan-nels slashed the cost of transporting both raw materials and finished goods.\\nRoad Transportation British roads improved, too, thanks largely to the efforts of\\nJohn McAdam, a Scottish engineer.'},\n",
       " {'corpus_id': 3,\n",
       "  'score': -11.189367,\n",
       "  'text': 'راه\\u200cآهن\\u200cها باعث تسهیل حمل و نقل و توسعه اقتصادی در ایران شدند.'},\n",
       " {'corpus_id': 5,\n",
       "  'score': -11.223364,\n",
       "  'text': 'صنعت نفت در ایران تاثیر بسیار زیادی بر اقتصاد داشت.'},\n",
       " {'corpus_id': 6,\n",
       "  'score': -11.240981,\n",
       "  'text': 'ورزش فوتبال در ایران محبوبیت زیادی دارد و تأثیر فرهنگی زیادی داشته است.'},\n",
       " {'corpus_id': 4,\n",
       "  'score': -11.268467,\n",
       "  'text': 'راه\\u200cآهن\\u200cها در دوران پهلوی نقش مهمی در گسترش تجارت و حمل\\u200cونقل ایفا کردند.'},\n",
       " {'corpus_id': 7,\n",
       "  'score': -11.417374,\n",
       "  'text': 'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.'},\n",
       " {'corpus_id': 9,\n",
       "  'score': -11.44614,\n",
       "  'text': 'In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.'},\n",
       " {'corpus_id': 8,\n",
       "  'score': -11.456753,\n",
       "  'text': 'Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821de063",
   "metadata": {},
   "source": [
    "MiniLM-L6-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8c2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5c4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: D:\\Projects\\venv2\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary D:\\Projects\\venv2\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n",
      "Query: How many people live in Berlin?\n",
      "- #0 (8.61): Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\n",
      "- #2 (6.35): In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\n",
      "- #1 (5.51): Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# 1. Load a pretrained CrossEncoder model\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "\n",
    "query = query_1\n",
    "passages = passages_1\n",
    "\n",
    "\n",
    "# 2a. Either predict scores pairs of texts\n",
    "# scores = reranker.predict([(query, passage) for passage in passages])\n",
    "# print(scores)\n",
    "\n",
    "# 2b. Or rank a list of passages for a query\n",
    "ranks = reranker.rank(query, passages, return_documents=True)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "for rank in ranks:\n",
    "    print(f\"- #{rank['corpus_id']} ({rank['score']:.2f}): {rank['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b9f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85f6a186",
   "metadata": {},
   "source": [
    "Qwen3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c125a139",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'qwen3'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the sequence classification reranker (no architecture mismatch)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m reranker \u001b[38;5;241m=\u001b[39m \u001b[43mCrossEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtomaarsen/Qwen3-Reranker-0.6B-seq-cls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Prepare your query and passage candidates as before\u001b[39;00m\n\u001b[0;32m      7\u001b[0m pairs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the effects of railroads in Britain\u001b[39m\u001b[38;5;124m\"\u001b[39m, doc)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m passages\n\u001b[0;32m     10\u001b[0m ]\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:74\u001b[0m, in \u001b[0;36mCrossEncoder.__init__\u001b[1;34m(self, model_name, num_labels, max_length, device, tokenizer_args, automodel_args, trust_remote_code, revision, local_files_only, default_activation_function, classifier_dropout)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m automodel_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     automodel_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m classifier_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load the sequence classification reranker (no architecture mismatch)\n",
    "reranker = CrossEncoder(\"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\")\n",
    "\n",
    "# Prepare your query and passage candidates as before\n",
    "pairs = [\n",
    "    (\"Explain the effects of railroads in Britain\", doc)\n",
    "    for doc in passages\n",
    "]\n",
    "scores = reranker.predict(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6046150",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'qwen3'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the classification reranker\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m reranker \u001b[38;5;241m=\u001b[39m \u001b[43mCrossEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtomaarsen/Qwen3-Reranker-0.6B-seq-cls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the effects of railroads in Britain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m candidate_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRailroads revolutionized transport and trade in Britain...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe industrial revolution and railroads helped...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome unrelated text here...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m ]\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:74\u001b[0m, in \u001b[0;36mCrossEncoder.__init__\u001b[1;34m(self, model_name, num_labels, max_length, device, tokenizer_args, automodel_args, trust_remote_code, revision, local_files_only, default_activation_function, classifier_dropout)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m automodel_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     automodel_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m classifier_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load the classification reranker\n",
    "reranker = CrossEncoder(\"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\")\n",
    "\n",
    "query = \"Explain the effects of railroads in Britain\"\n",
    "candidate_docs = [\n",
    "    \"Railroads revolutionized transport and trade in Britain...\",\n",
    "    \"The industrial revolution and railroads helped...\",\n",
    "    \"Some unrelated text here...\"\n",
    "]\n",
    "\n",
    "pairs = [(query, doc) for doc in candidate_docs]\n",
    "scores = reranker.predict(pairs)\n",
    "ranked = [doc for _, doc in sorted(zip(scores, candidate_docs), reverse=True)]\n",
    "\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Ranked docs:\", ranked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b515c2cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 757479 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQwen/Qwen3-Reranker-0.6B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen3-Reranker-0.6B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:896\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    893\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m         )\n\u001b[1;32m--> 896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    898\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2291\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2288\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2289\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2292\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2293\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2294\u001b[0m     init_configuration,\n\u001b[0;32m   2295\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2296\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2297\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2298\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2299\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2300\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2301\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2302\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2303\u001b[0m )\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2525\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2524\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2525\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2528\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2530\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\qwen2\\tokenization_qwen2_fast.py:120\u001b[0m, in \u001b[0;36mQwen2TokenizerFast.__init__\u001b[1;34m(self, vocab_file, merges_file, tokenizer_file, unk_token, bos_token, eos_token, pad_token, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m unk_token \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    110\u001b[0m     AddedToken(unk_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(unk_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m unk_token\n\u001b[0;32m    113\u001b[0m )\n\u001b[0;32m    114\u001b[0m pad_token \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     AddedToken(pad_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pad_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m pad_token\n\u001b[0;32m    118\u001b[0m )\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    121\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39mvocab_file,\n\u001b[0;32m    122\u001b[0m     merges_file\u001b[38;5;241m=\u001b[39mmerges_file,\n\u001b[0;32m    123\u001b[0m     tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[0;32m    124\u001b[0m     unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[0;32m    125\u001b[0m     bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    126\u001b[0m     eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[0;32m    127\u001b[0m     pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    129\u001b[0m )\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:115\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[1;31mException\u001b[0m: data did not match any variant of untagged enum ModelWrapper at line 757479 column 3"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c372f61",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 757479 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     scores \u001b[38;5;241m=\u001b[39m batch_scores[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[1;32m---> 33\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQwen/Qwen3-Reranker-0.6B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen3-Reranker-0.6B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# We recommend enabling flash_attention_2 for better acceleration and memory saving.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").cuda().eval()\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:896\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    893\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m         )\n\u001b[1;32m--> 896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    898\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2291\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2288\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2289\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2292\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2293\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2294\u001b[0m     init_configuration,\n\u001b[0;32m   2295\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2296\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2297\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2298\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2299\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2300\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2301\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2302\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2303\u001b[0m )\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2525\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2524\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2525\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2528\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2530\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\qwen2\\tokenization_qwen2_fast.py:120\u001b[0m, in \u001b[0;36mQwen2TokenizerFast.__init__\u001b[1;34m(self, vocab_file, merges_file, tokenizer_file, unk_token, bos_token, eos_token, pad_token, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m unk_token \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    110\u001b[0m     AddedToken(unk_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(unk_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m unk_token\n\u001b[0;32m    113\u001b[0m )\n\u001b[0;32m    114\u001b[0m pad_token \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     AddedToken(pad_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pad_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m pad_token\n\u001b[0;32m    118\u001b[0m )\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    121\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39mvocab_file,\n\u001b[0;32m    122\u001b[0m     merges_file\u001b[38;5;241m=\u001b[39mmerges_file,\n\u001b[0;32m    123\u001b[0m     tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[0;32m    124\u001b[0m     unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[0;32m    125\u001b[0m     bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    126\u001b[0m     eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[0;32m    127\u001b[0m     pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    129\u001b[0m )\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:115\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[1;31mException\u001b[0m: data did not match any variant of untagged enum ModelWrapper at line 757479 column 3"
     ]
    }
   ],
   "source": [
    "# Requires transformers>=4.51.0\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n",
    "    return output\n",
    "\n",
    "def process_inputs(pairs):\n",
    "    inputs = tokenizer(\n",
    "        pairs, padding=False, truncation='longest_first',\n",
    "        return_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "    )\n",
    "    for i, ele in enumerate(inputs['input_ids']):\n",
    "        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\n",
    "    inputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "    return inputs\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logits(inputs, **kwargs):\n",
    "    batch_scores = model(**inputs).logits[:, -1, :]\n",
    "    true_vector = batch_scores[:, token_true_id]\n",
    "    false_vector = batch_scores[:, token_false_id]\n",
    "    batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "    scores = batch_scores[:, 1].exp().tolist()\n",
    "    return scores\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").cuda().eval()\n",
    "token_false_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "max_length = 8192\n",
    "\n",
    "prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "        \n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "\n",
    "queries = [\"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "\n",
    "pairs = [format_instruction(task, query, doc) for query, doc in zip(queries, documents)]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = process_inputs(pairs)\n",
    "scores = compute_logits(inputs)\n",
    "\n",
    "print(\"scores: \", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "941f22d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.43.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: d:\\projects\\venv2\\lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: auto_gptq, autoawq, compressed-tensors, lm_eval, optimum, peft, sentence-transformers, tokenicer, TTS, vllm\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98399f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cd0ddff",
   "metadata": {},
   "source": [
    "Alibaba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8527a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.3.1+cu121 with CUDA 1201 (you have 2.3.1+cu118)\n",
      "    Python  3.10.11 (you have 3.10.9)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Projects\\venv2\\lib\\site-packages\\xformers\\__init__.py\", line 57, in _is_triton_available\n",
      "    import triton  # noqa\n",
      "ModuleNotFoundError: No module named 'triton'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NewForSequenceClassification(\n",
       "  (new): NewModel(\n",
       "    (embeddings): NewEmbeddings(\n",
       "      (word_embeddings): Embedding(250048, 768, padding_idx=1)\n",
       "      (rotary_emb): NTKScalingRotaryEmbedding()\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): NewEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x NewLayer(\n",
       "          (attention): NewSdpaAttention(\n",
       "            (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (mlp): NewGatedMLP(\n",
       "            (up_gate_proj): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act_fn): GELUActivation()\n",
       "            (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (mlp_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): NewPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"Alibaba-NLP/gte-multilingual-reranker-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    "    )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2153c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"How many people live in Berlin?\"\n",
    "passages_1 = [\n",
    "    \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\",\n",
    "    \"Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.\",\n",
    "    \"In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\",\n",
    "]\n",
    "\n",
    "query_2 = \"Explain the effects of railroads in Britain\"\n",
    "passages_2 = [\n",
    "    \"Railroads revolutionized transport and trade in Britain...\",\n",
    "    \"The industrial revolution and railroads helped...\",\n",
    "    \"Some unrelated text here...\"\n",
    "]\n",
    "\n",
    "query_3 = \"تاثیرات راه‌آهن‌ها در اقتصاد ایران چیست؟\"\n",
    "\n",
    "passages_3 = [\n",
    "    \"راه‌آهن‌ها باعث تسهیل حمل و نقل و توسعه اقتصادی در ایران شدند.\",\n",
    "    \"صنعت نفت در ایران تاثیر بسیار زیادی بر اقتصاد داشت.\",\n",
    "    \"ورزش فوتبال در ایران محبوبیت زیادی دارد و تأثیر فرهنگی زیادی داشته است.\",\n",
    "    \"راه‌آهن‌ها در دوران پهلوی نقش مهمی در گسترش تجارت و حمل‌ونقل ایفا کردند.\"\n",
    "]\n",
    "\n",
    "query_4 = \"تاثیرات راه‌آهن‌ها در اقتصاد ایران چیست؟\"\n",
    "\n",
    "passages_4 = [\n",
    "    # Relevant - Persian\n",
    "    \"\"\"راه‌آهن‌ها باعث تسهیل حمل‌ونقل کالا و مسافر شده و نقش مهمی \n",
    "    در کاهش هزینه‌های حمل‌ونقل و افزایش سرعت تجارت بین استان‌ها داشته‌اند. \n",
    "    اتصال بنادر جنوبی به مراکز صنعتی شمال کشور نیز از دستاوردهای مهم آن بوده است.\"\"\",\n",
    "\n",
    "    # Relevant - English\n",
    "    \"\"\"Railways in Iran have significantly reduced transportation costs, \n",
    "    boosted intercity trade, and connected industrial hubs to major ports, \n",
    "    leading to increased economic growth.\"\"\",\n",
    "\n",
    "    # Irrelevant - Persian\n",
    "    \"\"\"صنعت نفت در ایران نقش کلیدی در اقتصاد داشته اما ارتباط مستقیمی با حمل‌ونقل ریلی ندارد.\"\"\",\n",
    "\n",
    "    # Irrelevant - English\n",
    "    \"\"\"The oil industry in Iran has been a major driver of GDP but is unrelated to rail transport.\"\"\",\n",
    "\n",
    "    # Irrelevant - Persian (different domain)\n",
    "    \"\"\"ورزش فوتبال در ایران محبوبیت زیادی دارد و تأثیر فرهنگی قابل توجهی بر جامعه گذاشته است.\"\"\",\n",
    "\n",
    "    # Relevant - Persian (historical context)\n",
    "    \"\"\"در دوران پهلوی اول، توسعه خطوط راه‌آهن به‌ویژه مسیر شمال-جنوب \n",
    "    باعث افزایش حجم صادرات و واردات و رونق بازارهای محلی شد.\"\"\",\n",
    "\n",
    "    # Relevant - English (historical)\n",
    "    \"\"\"During the Pahlavi era, the expansion of the north-south railway line \n",
    "    played a major role in boosting trade and connecting rural markets to major cities.\"\"\",\n",
    "\n",
    "    # Partially relevant - English\n",
    "    \"\"\"The industrial revolution and railroads helped shape the economies of many countries, \n",
    "    including modernizing transportation in Iran.\"\"\",\n",
    "\n",
    "    # Irrelevant filler - English\n",
    "    \"\"\"Some unrelated text here about cooking recipes and healthy diets...\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70d5ba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9268,  0.2771, -3.1543, -3.3008, -3.3164,  0.1852, -0.4709, -2.2480,\n",
      "        -3.2676])\n"
     ]
    }
   ],
   "source": [
    "query = query_4\n",
    "passages = passages_4\n",
    "\n",
    "pairs = [(query, doc) for doc in passages]\n",
    "pairs\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alibab reranker wrap up\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "reranker_name = \"Alibaba-NLP/gte-multilingual-reranker-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(reranker_name)\n",
    "reranker = AutoModelForSequenceClassification.from_pretrained(\n",
    "    reranker_name, trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    "    )\n",
    "reranker.eval()\n",
    "\n",
    "\n",
    "query = query_4\n",
    "passages = passages_4\n",
    "\n",
    "pairs = [(query, doc) for doc in passages]\n",
    "pairs\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    scores = reranker(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b1911",
   "metadata": {},
   "source": [
    "Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3916097",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'qwen3'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m----> 3\u001b[0m reranker \u001b[38;5;241m=\u001b[39m \u001b[43mCrossEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtomaarsen/Qwen3-Reranker-0.6B-seq-cls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m scores \u001b[38;5;241m=\u001b[39m reranker\u001b[38;5;241m.\u001b[39mpredict([\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the effects of railroads in Britain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRailroads revolutionized transport...\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      7\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the effects of railroads in Britain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe industrial revolution and railroads helped...\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      8\u001b[0m ])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScores:\u001b[39m\u001b[38;5;124m\"\u001b[39m, scores)\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:74\u001b[0m, in \u001b[0;36mCrossEncoder.__init__\u001b[1;34m(self, model_name, num_labels, max_length, device, tokenizer_args, automodel_args, trust_remote_code, revision, local_files_only, default_activation_function, classifier_dropout)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m automodel_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     automodel_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m classifier_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Projects\\venv2\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder(\"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\")\n",
    "\n",
    "scores = reranker.predict([\n",
    "    (\"Explain the effects of railroads in Britain\", \"Railroads revolutionized transport...\"),\n",
    "    (\"Explain the effects of railroads in Britain\", \"The industrial revolution and railroads helped...\"),\n",
    "])\n",
    "print(\"Scores:\", scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
