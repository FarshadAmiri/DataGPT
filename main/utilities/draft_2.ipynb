{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa9a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.chdir(\"D:\\Projects\\RAG-webapp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a26b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name=\"TheBloke/Llama-2-7b-Chat-GPTQ\", device='gpu'):\n",
    "    # setting device\n",
    "    if device == 'gpu':\n",
    "        gpu=0\n",
    "        device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_device(device)\n",
    "        torch.cuda.get_device_name(0)\n",
    "    elif device == 'cpu':\n",
    "        device = torch.device('cpu')\n",
    "        torch.cuda.set_device(device)\n",
    "\n",
    "    with open('huggingface_credentials.txt', 'r') as file:\n",
    "        hf_token = file.readline().strip()\n",
    "\n",
    "    login(token=hf_token)\n",
    "\n",
    "    # Create tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name\n",
    "        ,device_map='cuda'                 \n",
    "        )\n",
    "\n",
    "    # Define model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name\n",
    "        # ,cache_dir=r\"C:\\Users\\henry\\.cache\\huggingface\\hub\"\n",
    "        # ,cache_dir=r\"C:\\Users\\user2\\.cache\\huggingface\\hub\"\n",
    "        ,device_map='cuda'  \n",
    "        # , torch_dtype=torch.float16\n",
    "        # ,low_cpu_mem_usage=True\n",
    "        # ,rope_scaling={\"type\": \"dynamic\", \"factor\": 2}\n",
    "        # ,load_in_8bit=True,\n",
    "        ).to(device)\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    model_obj = {\"model\": model, \"tokenizer\": tokenizer, \"streamer\": streamer, \"device\": device,  }\n",
    "\n",
    "    return model_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d4dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: D:\\Projects\\venv2\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary D:\\Projects\\venv2\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\venv2\\lib\\site-packages\\awq\\modules\\linear\\exllama.py:12: UserWarning: AutoAWQ could not load ExLlama kernels extension. Details: DLL load failed while importing exl_ext: The specified module could not be found.\n",
      "  warnings.warn(f\"AutoAWQ could not load ExLlama kernels extension. Details: {ex}\")\n",
      "D:\\Projects\\venv2\\lib\\site-packages\\awq\\modules\\linear\\exllamav2.py:13: UserWarning: AutoAWQ could not load ExLlamaV2 kernels extension. Details: DLL load failed while importing exlv2_ext: The specified module could not be found.\n",
      "  warnings.warn(f\"AutoAWQ could not load ExLlamaV2 kernels extension. Details: {ex}\")\n",
      "D:\\Projects\\venv2\\lib\\site-packages\\awq\\modules\\linear\\gemm.py:14: UserWarning: AutoAWQ could not load GEMM kernels extension. Details: DLL load failed while importing awq_ext: The specified module could not be found.\n",
      "  warnings.warn(f\"AutoAWQ could not load GEMM kernels extension. Details: {ex}\")\n",
      "D:\\Projects\\venv2\\lib\\site-packages\\awq\\modules\\linear\\gemv.py:11: UserWarning: AutoAWQ could not load GEMV kernels extension. Details: DLL load failed while importing awq_ext: The specified module could not be found.\n",
      "  warnings.warn(f\"AutoAWQ could not load GEMV kernels extension. Details: {ex}\")\n",
      "D:\\Projects\\venv2\\lib\\site-packages\\awq\\modules\\linear\\gemv_fast.py:10: UserWarning: AutoAWQ could not load GEMVFast kernels extension. Details: DLL load failed while importing awq_v2_ext: The specified module could not be found.\n",
      "  warnings.warn(f\"AutoAWQ could not load GEMVFast kernels extension. Details: {ex}\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"\n",
    "\n",
    "model_obj = load_model(model_name)\n",
    "model = model_obj[\"model\"]\n",
    "tokenizer = model_obj[\"tokenizer\"]\n",
    "device = model_obj[\"device\"]\n",
    "streamer = model_obj[\"streamer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48cd422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "        You are a helpful, respectful and honest assistant. Always answer as\n",
    "        helpfully as possible, while being safe.`\n",
    "        If a question does not make any sense, or is not factually coherent, explain\n",
    "        why instead of answering something not correct. If you don't know the answer\n",
    "        to a question, please don't share false information.\n",
    "        Try to be exact in information and numbers you tell.\n",
    "        Your goal is to provide answers completely based on the information provided\n",
    "        and if you use yourown knowledge please inform the user.\n",
    "        and it is important to respond as breifly as possible.<</SYS>>\n",
    "        # \"\"\"\n",
    "\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=512,\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the response object (with streaming enabled in your query engine)\n",
    "response = self.query_engine.query(msg)\n",
    "\n",
    "# The response object has a .response_gen generator for streaming tokens/chunks\n",
    "try:\n",
    "    response_gen = response.response_gen\n",
    "except AttributeError:\n",
    "    # Fallback if streaming is not enabled or not supported\n",
    "    yield \"%%%END%%%\"\n",
    "    return\n",
    "\n",
    "# Stream the response token by token/chunk by chunk\n",
    "try:\n",
    "    while True:\n",
    "        chunk = next(response_gen)\n",
    "        # Do something with chunk, e.g., send to websocket\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        # In async context, you might want to: await asyncio.sleep(0)\n",
    "except StopIteration:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1f63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2bdc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a568f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
